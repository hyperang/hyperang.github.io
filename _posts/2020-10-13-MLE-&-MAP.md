---
layout: post
title:  "最大似然估计与最大后验概率估计"
date:   2020-10-13 00:07:00 +0800
tags: 统计学
color: rgb(255,90,90)
cover: '../assets/MLE_&_MAP/1.png'
subtitle: 'MLE & MAP'
---



本文主要整理了[nebulaf91][link-1]的[文章][link-2]中的内容。

[link-1]: http://blog.csdn.net/u011508640/
[link-2]: http://blog.csdn.net/u011508640/article/details/72815981



## 概率和统计的区别

概率与统计研究的问题正好相反。

概率研究的问题是已知一个模型和参数，预测模型产生的结果（数据）。

统计研究的问题是已有一堆数据，通过数据预测模型和参数。



## 贝叶斯公式

首先来看条件概率公式：

$$
P(A|B) = \frac{P(AB)}{P(B)}
$$


贝叶斯公式：

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$


将分母上的$P(B)$展开：

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\overline{A})P(\overline{A})}
$$



贝叶斯公式可以理解为在描述：

**有多大把握能相信一件证据？（how much you can trust the evidence）**



假设汽车响警报的目的是想告知车主*汽车被砸了*。事件$A$为*汽车被砸了*，事件$B$为*警报响了*。

带进贝叶斯公式里看，等式左边表示*警报响了时汽车被砸*的概率。即警报已经响了，有多大概率汽车被砸了。

等式右边分母分成了*警报响起汽车被砸*和*警报响起汽车没被砸*的和。

如果要让
$$
{P(A|B)}=1
$$
那么就需要杜绝其他情况，即:
$$
{P(B|\overline A)}{P(\overline A)}=0
$$


从此角度解释贝叶斯公式：

**做判断的时候，要考虑所有的因素**



如果汽车被砸的概率很小，那么等式分子很小（假设被砸一定响警报，即使
$$
P(B|A)=1
$$
依然如此）。那么
$$
P(A|B)
$$
依然会小。

这里
$$
P(A)
$$
即为先验概率。在
$$
{P(B|\overline A)}{P(\overline A)}
$$
不变的情况下，如果先验概率很小，即使
$$
{P(B|A)}
$$
很大，
$$
A
$$
的后验概率
$$
{P(A|B)}
$$
也不会大。

从此角度解释贝叶斯公式：

**一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情**



## 似然函数

对于函数：
$$
{P(x;\theta)}
$$
$x$表示一个具体的数据，$\theta$表示模型的参数。

如果$\theta$是已知的，$x$是变量，这个函数叫做概率函数（probability function），描述对于不同的样本点$x$，其出现的概率。

如果$x$是已知的，$\theta$是变量，这个函数叫做似然函数（likelihood function），它描述对于不同的模型参数，出现$x$这个样本点的概率。



## 最大似然估计（MLE）

假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率各是多少。

这是一个统计问题，解决统计问题需要数据。

于是我们拿这枚硬币抛了10次，得到的数据是：反正正正正反正正正反。

假设数据为$x_0$。

抛硬币的模型为二项分布：
$$
X \sim B(n,p)
$$
则要求解的正面出现的概率$\theta$为模型参数。

数据$x_0$的似然函数为：
$$
f(x_0 ,\theta) = (1-\theta)\times\theta\times\theta\times\theta\times\theta\times(1-\theta)\times\theta\times\theta\times\theta\times(1-\theta) = \theta ^ 7(1 - \theta)^3 = f(\theta)
$$
似然函数只和$\theta$相关。

最大似然估计（Maximum likelihood estimation, 简称MLE）即要最大化这个函数的值。

$f(\theta)$的图像为：

![png]({{site.url}}\assets\MLE_&_MAP\1.png)

可以看出，在$f(\theta)$在$\theta$为$0.7$时取得最大值，即抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是$0.7$。



总结一下，最大似然估计完全以完全依赖观测集的数据，**即频率派的观点**。假设数据集中的数据独立同分布，求使得数据集$X$的似然函数最大的$\theta$。通常采用最大对数似然，即：
$$
\theta _{MLE} = \mathop{\arg\max}_{\theta} \log p(X|\theta)\mathop{=}_{iid} \mathop{\arg\max}_{\theta} \sum_{n=1}^N \log p(x_i|\theta)
$$




## 最大后验概率估计（MAP）

最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是要求$\theta$使
$${P(x_0|\theta)}{P(\theta)}$$
最大。

最大后验概率估计其实是在最大化

$$
{P(\theta|x_0)} = \frac{P(x_0|\theta)P(\theta)}{P(x_0)}
$$

式中$x_0$是确定的，即投出的“反正正正正反正正正反”，故$P(x_0)$确定。

最大化
$${P(\theta|x_0)}$$
的意义也很明确，$x_0$已经出现（确定）了，求$\theta$取什么值时
$${P(\theta|x_0)}$$
最大。

因为$P(x_0)$确定，故在计算时可以忽略，即:

$$
\mathop{\arg\max}_{\theta}P(\theta|x_0)=\mathop{\arg\max}_{\theta}P(x_0|\theta)P(\theta)
$$


$${P(\theta|x_0)}$$
即后验概率。

显然后验概率使用了贝叶斯定理，即**贝叶斯派的观点**。

$${P(x_0|\theta)}$$
即似然函数。

对于投硬币的例子，我们认为（”先验地知道“）$\theta$取$0.5$的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设$P(\theta)$满足期望$0.5$，方差$0.01$的高斯分布，即：

$$
P(\theta)\sim N(0.5,0.01)
$$

有：

$$
\mathop{\arg\max}_{\theta}P(\theta|x_0)=\mathop{\arg\max}_{\theta}P(x_0|\theta)P(\theta)=\mathop{\arg\max}_{\theta} \theta ^ 7(1 - \theta)^3 \frac{1}{\sqrt{0.2\pi}}e^{-\frac{(\theta-0.5)^2}{0.02}}
$$



$P(\theta)$的函数图像为：

![png]({{site.url}}\assets\MLE_&_MAP\2.png)

$${P(x_0|\theta)}P(\theta)$$
的函数图像为：

![png]({{site.url}}\assets\MLE_&_MAP\3.png)

此时函数取最大值时，$\theta$取值已向左偏移，不再是$0.7$。实际上，在$\theta = 0.558$时函数取得了最大值。即，用最大后验概率估计，得到$\theta = 0.558$。



## MLE 与 MAP 的关系

怎样才能说服一个贝叶斯派相信$\theta = 0.7$呢？

如果做了$1000$次实验，其中$700$次都是正面向上，这时似然函数为:

![png]({{site.url}}\assets\MLE_&_MAP\4.png)

如果仍然假设$P(\theta)$为均值$0.5$，方差$0.01$的高斯函数，
$${P(x_0 | \theta)} {P(\theta)}$$
的函数图像为：

![png]({{site.url}}\assets\MLE_&_MAP\5.png)

在$\theta = 0.696$处，
$${P(x_0 | \theta)} P(\theta)$$
取得最大值。

这样，就算一个考虑了先验概率的贝叶斯派，也不得不承认得把$\theta$估计在$0.7$附近了。

要是遇上了顽固的贝叶斯派，认为$P(\theta = 0.5) = 1$ ，那么无论怎么做实验，使用 MAP 估计出来都是$\theta = 0.5$。这也说明，一个合理的先验概率假设是很重要的。通常，先验概率能从数据中直接分析得到。