---
layout: post
title:  "熵和K-L散度"
date:   2020-10-13 15:35:10 +0800
tags: 信息论
color: rgb(255,90,90)
cover: '../assets/entropy/Shannon.jpg'
subtitle: '信息论中的几个基础概念'
---



## 信息熵（entropy）

熵是随机变量不确定度的度量（a measure of the uncertainty of a random variable）。

$$X$$
是一个离散型随机变量（discrete random variable），其取值空间（alphabet）为
$$\chi$$
，其概率密度函数
$$p(x)=\Pr \{ X=x\},x\in \chi$$
，此处简写了概率密度函数
$p_X(x)$为$p(x)$
。

$X$的熵$H(X)$定义为：

$$
H(X)=-\sum_{x\in\mathscr{X}}p(x)\log p(x)
$$

对数底为$2$，单位为比特。

若对数底为$e$，单位为奈特（nat）。

当$x\rightarrow 0$时，$x \log x \rightarrow 0$，故约定$0 \log 0 = 0$，即加上零概率的项不改变熵的值。

熵是随机变量$X$的分布的函数（entropy is a functional of the distribution of $X$），它并不依赖$X$的实际取值，而仅依赖于$X$的概率分布。



当$X$只能取$0,1$，$X\sim Bern(p)$，即：

$$
X = \left\{
	\begin{array}{lr}
		1& \text{with probability}&p\\
		0&\text{with probability}&1-p
	\end{array}
\right.
$$

$X$的熵为：

$$
H(X)=-p\log p - (1-p)\log (1-p)
$$

$H(X)$的图像为：

![png]({{site.url}}\assets\entropy\1.png)

当$p=0$或$p=1$时，$H(X)=0$，随机变量完全没有不确定性。

当$p=0.5$时，$H(X)=1$，随机变量的不确定性最大。



用$E$表示数学期望。

$X \sim p(x)$，则随机变量$g(X)$的期望为：

$$
E_pg(X)=\sum_{x \in \mathscr{X}}g(x)p(x)
$$

简记为$E_g (X)$。

令$g(X)=-\log P(X)$，此时$E_g (X) = H(X)$，即：

$X$的熵可以解释为$-\log P(X)$的期望。





## 联合熵（joint entropy）

对于服从联合分布为$p(x,y)$的一对离散随机变量$(X,Y)$，其联合熵$H(X,Y)$定义为：

$$
H(X,Y)=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(x,y)
$$

写成分布的形式为：

$$
H(X,Y)=-E\log p(X,Y)
$$


## 条件熵（conditional entropy）

对于服从联合分布为$p(x,y)$的一对离散随机变量$(X,Y)$，其条件熵
$H(Y|X)$
定义为：
$$
\begin{align}
H(Y|X)&=\sum_{x \in \mathscr{X}}p(x)H(Y|X=x)\\
&=-\sum_{x \in \mathscr{X}}p(x)\sum_{y \in \mathscr{Y}}p(y|x)\log p(y|x)\\
&=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(y|x)\\
&=-E\log p(Y|X)
\end{align}
$$


联合熵与条件熵的链式法则：

$$
\begin{align}
H(X,Y)&=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(x,y)\\
&=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(x)p(y|x)\\
&=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(x)-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(y|x)\\
&=-\sum_{x \in \mathscr{X}}p(x)\log p(x)-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(y|x)\\
&=H(X)+H(Y|X)

\end{align}
$$

推导过程中的条件概率公式：

$$
p(x,y)=p(x|y)p(y)=p(y|x)p(x)
$$


## 相对熵（relative entropy）

相对熵是两个随机分布之间的距离的度量（The *relative entropy* is a measure of the distance between two distributions）。

$p(x)$和$q(x)$是两个概率密度函数，则$p(x)$和$q(x)$间的相对熵或 *Kullback–Leibler distance*定义为： 

$$
\begin{align}
D(p||q)&=\sum_{x \in \mathscr{X}}p(x)\log \frac {p(x)}{q(x)}\\
	   &=E_p\log \frac{p(X)}{q(X)}
\end{align}
$$

亦可写为：

$$
\begin{align}
D_{KL}(p||q)&= \sum_{x \in \mathscr{X}}p(x)(\log p(x)-\log q(x)) \\
		    &=E_p[\log p(X)-\log q(X)]
\end{align}
$$



相对熵可理解为似然比的对数期望。它度量当真实分布为$p$而假定分布为$q$时的无效性。

相对熵非负且当且仅当$p=q$时为零。

相对熵不对称（
$$D(p||q)\neq D(q||p)$$
）且不满足三角不等式，故它并非两个分布之间的真正距离。

*Kullback–Leibler distance*并不准确，应为K-L散度，即*Kullback–Leibler divergence*。





## 互信息（mutual information）

考虑两个随机变量$X$和$Y$，它们的联合概率密度函数为$p(x,y)$，其边际概率密度函数分别为$p(x)$和$p(y)$。互信息$I(X;Y)$为联合分布$p(x,y)$和乘积分布$p(x)p(y)$之间的相对熵：

$$
\begin{align}
I(X;Y)&=D(p(x,y)||p(x)p(y))\\
&=\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
&=E_{p(x,y)}\log \frac{p(X,Y)}{p(X)p(Y)}
\end{align}
$$


## 熵与互信息的关系

可以将互信息$I(X;Y)$重新写为：

$$
\begin{align}
I(X,Y)&=\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
&=\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y) \log \frac{p(x|y)p(y)}{p(x)p(y)}\\
&=\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y) \log \frac{p(x|y)}{p(x)}\\
&=-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y) \log p(x)-(-\sum_{x \in \mathscr{X}}\sum_{y \in \mathscr{Y}}p(x,y)\log p(x|y))\\
&=H(X)-H(X|Y)

\end{align}
$$

互信息$I(X;Y)$可以理解为在给定$Y$知识的条件下$X$的不确定度的缩减量。

对称地：

$$
I(X,Y)=H(Y)-H(Y|X)
$$

$X$含有$Y$的信息量与$Y$含有$X$的信息量相同。



随机变量与自身的互信息为该随机变量的熵：

$$
I(X;X)=H(X)-H(X|X)=H(X)
$$

故熵也称为自信息（self-information）。



## 信息熵与信息量

这里厘清几个很容易混淆的概念。

首先思考一个问题，一个随机变量的熵越大，它的信息量是越大还是越小？

熵即自信息，于是自然地有：**熵越大信息量越大**。如何在现实中理解这个解释呢？这里的信息量是指信息的承载能力。如果一个多态系统只有一个态概率为$1$，那么它的熵为$0$，这个系统也无法承载任何信息（无法进行编码）。如果它的多个状态等概率，此时熵最大，信息的承载能力越强。

假设一个考试的场景，考卷上出现了一道你完全没有见过的选择题，此时你选择问一下坐在你前面的人，他回答说每个选项正确的概率都相同。他的回答熵是最大的，但是他的回答不包含任何的信息！你还是不会做这道题。这又是怎么一回事儿呢？

这是因为上面场景中的信息量和之前信息量的定义不同。

此时的信息量可以改称为知识，是不确定度的缩减量，即互信息。他的回答中并不包含任何的知识，不能帮你做这道题。如果你又问了坐在你后面的人，她的回答是八成概率选 A，不可能选 D，她的回答让你可以先在答题卡上写A了，假设她的话可信，那么你对这道题的认识改变了，这个系统的熵减小了。

知识的输入使得这个问题的信息熵减小了。如果有第三个人跟你说这道题选D，那么他的话就毫无意义了（假设坐在你后面的人的话是可信的）。如果你本身就会做这道题，那么这道题对你来说变成了一个熵为$0$的系统，则所有人对你说的关于这道题的话都没有意义了（熵已经不会再减小了）。





## 连续随机变量的熵

考虑一个定义在$[a,b]$的连续随机变量$X$，其概率密度函数为$p(x)$，有$\int_a^b p(x)d(x)=1$。

把$X$的取值区间$[a,b]$等分为$n$个小区间，小区间宽度为$\triangle =\frac{(b-a)}{n}$，则$X$取到第$i$个小区间$x_i$的概率为$p(x_i)\triangle$，$x_i$为第$i$个小区间中的一点，于是有分割后的离散随机变量$X_n$的分布：



| $x_1$             | $x_2$             | ...  | $x_n$             |
| ----------------- | ----------------- | ---- | ----------------- |
| $p(x_1)\triangle$ | $p(x_2)\triangle$ | ...  | $p(x_n)\triangle$ |



由离散熵的定义可得：

$$
\begin{align}
H(X_n) &= -\sum_{i=1}^n [p(x_i)\triangle] \log [p(x_i)\triangle]\\
&=-\sum_{i=1}^n p(x_i)\triangle \log p(x_i) -\sum_{i=1}^n p(x_i)\triangle \log \triangle\\
&=-\sum_{i=1}^n p(x_i)\triangle \log p(x_i) - \log \triangle

\end{align}
$$


当$n\rightarrow \infty$，$\triangle \rightarrow 0$时，$X_n$接近于连续随机变量$X$，此时：

$$
\begin{align}
H(X)&=\lim_{\triangle \rightarrow 0\\n\rightarrow \infty} H(X_n)\\
&=\lim_{\triangle \rightarrow 0\\n\rightarrow \infty}\{-\sum_{i=1}^n p(x_i)\triangle \log p(x_i) - \log \triangle\}\\
&=-\int_a^bp(x)\log p(x)dx-\lim_{\triangle \rightarrow 0}\log \triangle\\
&=H_C(X)-\infty
\end{align}
$$


$H_C(X)$即为连续随机变量$X$的熵，也叫**微分熵(differential entropy)**。

假设$X\sim N(\mu,\sigma ^2)$，$X$的熵为：

$$
\begin{align}
H_C(X)&=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma ^2}}[\ln \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma ^2}(x-\mu)^2]dx\\
&=\ln\sqrt{2\pi}\sigma+\frac{1}{2}\\
&=\frac{1}{2}\ln 2\pi e\sigma ^2

\end{align}
$$

$H_C(X)$仅与方差有关，与期望无关。

**$X$的熵并不依赖它的实际取值**。



## 连续分布的相对熵

$f(x),g(x)$为两个连续分布的概率密度函数，$f(x),g(x)$的相对熵定义为：
$$
D_{KL}=\int f(x) \log\frac{f(x)}{g(x)}dx
$$


## 相对熵非负的证明

假设对$f(x)$有：

$$
\int_{-\infty}^{+\infty}f(x)dx=1
$$


如果$g$时任意实可测函数且函数$\phi$为凸，那么有**Jensen不等式**如下：

$$
\phi(\int_{-\infty}^{+\infty}g(x)f(x)dx)\leq\int_{-\infty}^{+\infty}\phi(g(x))f(x)dx
$$


$-\ln x$是严格凸函数。

且$\int q(x)dx=1$。

令$\phi (x)=- \ln (x)$，$g(x)=\frac{q(x)}{p(x)}$，$f(x)=p(x)$。

有：

$$
\begin{align}
D_{KL}&=\int f(x) \ln\frac{p(x)}{q(x)}dx\\
&=\int f(x) {-\ln\frac{q(x)}{p(x)}}dx\\
&\geq-\ln [\int q(x)dx]\\&=0
\end{align}
$$

当且仅当$p(x)=q(x)$时，等号成立。

